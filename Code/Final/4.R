library(dplyr)

input <- data.frame("x1" = c(9, 12, 17, 25, 7, 23, 27, 16, 15, 3, 5, 20, 13, 8, 18, 28, 11, 24, 6, 30, 10, 21, 29, 2, 14),
                    "x2" = c(7, 12, 11, 7, 14, 14, 7, 13, 10, 10, 10, 8, 12, 7, 9, 13, 14, 8, 10, 6, 11, 9, 13, 7, 9),
                    "y" = c(26.42, 39.44, 46.57, 52.09, 34.85, 60.54, 56.56, 48.81, 40.36, 23.46, 24.68, 45.53, 
                              40.63, 26.02, 45.08, 66.78, 40.16, 52.47, 28.26, 60.02, 34.42, 48.83, 68.50, 14.41, 37.98))

x <- cbind(1, input$x1, input$x2) %>% as.matrix(.)
y <- input$y %>% as.matrix(.)

# 학습률, 반복횟수
alpha <- 0.7
num_iters <- 5000


# adagrad 알고리즘을 위한 h 선언
# 그리고 동적 weight를 적용하기 위함
h <- w <- matrix(x %>% ncol(.) %>% rep(0, .))

# gradient descent
for (i in 1:num_iters) {
  
  grad <- t(x) %*% (x %*% w - y) / length(y)
  h <- h + grad^2
  w <- w - alpha * (h^(-1/2)) * grad 
}

w %>% cat(.)

